{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 03: Rerank - \"The Precision Logic\"\n",
        "\n",
        "## Innocenti Risk Management Enablement Kit\n",
        "\n",
        "---\n",
        "\n",
        "### The Problem\n",
        "\n",
        "Vector search finds semantically similar documents, but it doesn't understand **nuance**:\n",
        "\n",
        "- \"Can police use facial recognition?\" matches both:\n",
        "  - Article 5 (General prohibition)\n",
        "  - Article 5.1.d (Specific exceptions for law enforcement)\n",
        "\n",
        "A lawyer needs the **specific exception**, not just \"similar\" documents.\n",
        "\n",
        "### The Solution: Listwise Reranking\n",
        "\n",
        "**Traditional Reranking (Pointwise):**\n",
        "- Scores each document independently against the query\n",
        "- Doesn't consider relationships between documents\n",
        "\n",
        "**Listwise Reranking (Jina Reranker v3):**\n",
        "- Documents \"attend\" to each other during scoring\n",
        "- Understands context: \"This document is more specific than that one\"\n",
        "- The \"Lawyer's Brain\" effect\n",
        "\n",
        "**What we'll do:**\n",
        "1. Retrieve top 50 results (\"naive\" retrieval)\n",
        "2. Apply Jina Reranker v3 via Elasticsearch's `retrievers` API\n",
        "3. Compare before/after rankings\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q elasticsearch python-dotenv pandas\n",
        "\n",
        "from utils.colab_setup import setup_environment\n",
        "IN_COLAB = setup_environment(packages=\"elasticsearch python-dotenv pandas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "from elasticsearch import Elasticsearch, BadRequestError\n",
        "import pandas as pd\n",
        "\n",
        "# Import our credential helper\n",
        "# (Path is already set correctly in previous cell for both Colab and local)\n",
        "from utils.credentials import setup_notebook, get_index_name, get_inference_id, get_elasticsearch_client\n",
        "\n",
        "# Better pandas display\n",
        "pd.set_option('display.max_colwidth', 60)\n",
        "pd.set_option('display.width', 200)\n",
        "\n",
        "print(\"✓ Libraries loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup credentials and display configuration\n",
        "creds = setup_notebook(require_elastic=True, require_jina=False)\n",
        "\n",
        "# Get unique names for this user\n",
        "INDEX_NAME = get_index_name(\"search-eu-ai-act\")\n",
        "EMBEDDING_ID = get_inference_id(\"embeddings\")\n",
        "RERANKER_ID = get_inference_id(\"reranker\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Connect to Elasticsearch (works with both Cloud and Serverless)\n",
        "es = get_elasticsearch_client(creds)\n",
        "\n",
        "# Verify connection and index\n",
        "info = es.info()\n",
        "print(f\"✓ Connected to Elasticsearch {info['version']['number']}\")\n",
        "\n",
        "if es.indices.exists(index=INDEX_NAME):\n",
        "    count = es.count(index=INDEX_NAME)[\"count\"]\n",
        "    print(f\"✓ Index {INDEX_NAME} has {count} documents\")\n",
        "else:\n",
        "    raise ValueError(f\"Index {INDEX_NAME} not found. Run Notebook 02 first.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Reranker Inference Endpoint\n",
        "\n",
        "We need to set up the Jina Reranker v3 model as an inference endpoint.\n",
        "\n",
        "**Jina Reranker v3 highlights:**\n",
        "- **Listwise** attention mechanism\n",
        "- Documents attend to each other, not just the query\n",
        "- Better at distinguishing \"general\" vs \"specific\" matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from utils.inference import create_reranker_inference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create the reranker inference endpoint\n",
        "create_reranker_inference(es, RERANKER_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Our Test Query\n",
        "\n",
        "We'll use a nuanced legal query that has both general and specific answers in the EU AI Act."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# A nuanced query where context matters\n",
        "QUERY = \"Can law enforcement use facial recognition in public spaces?\"\n",
        "\n",
        "# What we expect:\n",
        "# - Article 5: General prohibition on biometric systems\n",
        "# - Article 5.1.d: EXCEPTIONS for law enforcement (the nuanced answer)\n",
        "\n",
        "print(f\"Test Query: \\\"{QUERY}\\\"\")\n",
        "print(\"\\nExpected behavior:\")\n",
        "print(\"  - Vector search: Returns general prohibition articles\")\n",
        "print(\"  - Reranked: Should surface the law enforcement EXCEPTIONS\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Naive Retrieval (Vector Search Only)\n",
        "\n",
        "First, let's see what pure semantic search returns - no reranking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def search_naive(es_client, index: str, query: str, size: int = 10) -> list:\n",
        "    \"\"\"\n",
        "    Perform naive semantic search without reranking.\n",
        "    \n",
        "    Args:\n",
        "        es_client: Elasticsearch client\n",
        "        index: Index name\n",
        "        query: Search query\n",
        "        size: Number of results\n",
        "    \n",
        "    Returns:\n",
        "        List of hit dictionaries\n",
        "    \"\"\"\n",
        "    results = es_client.search(\n",
        "        index=index,\n",
        "        query={\n",
        "            \"semantic\": {\n",
        "                \"field\": \"text\",\n",
        "                \"query\": query\n",
        "            }\n",
        "        },\n",
        "        size=size,\n",
        "        source=[\"article_number\", \"title\", \"text\"],\n",
        "    )\n",
        "    return results['hits']['hits']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "# Get naive results\n",
        "_t0 = time.time()\n",
        "naive_results = search_naive(es, INDEX_NAME, QUERY, size=10)\n",
        "naive_elapsed_ms = (time.time() - _t0) * 1000\n",
        "\n",
        "print(f\"--- Naive Semantic Search Results ({naive_elapsed_ms:.0f} ms) ---\")\n",
        "print(f\"Query: \\\"{QUERY}\\\"\\n\")\n",
        "\n",
        "naive_data = []\n",
        "for i, hit in enumerate(naive_results, 1):\n",
        "    naive_data.append({\n",
        "        \"Rank\": i,\n",
        "        \"Article\": hit['_source']['article_number'],\n",
        "        \"Title\": hit['_source']['title'][:50] + \"...\" if len(hit['_source']['title']) > 50 else hit['_source']['title'],\n",
        "        \"Score\": f\"{hit['_score']:.4f}\"\n",
        "    })\n",
        "\n",
        "df_naive = pd.DataFrame(naive_data)\n",
        "print(df_naive.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranked Retrieval (Using `retrievers` API)\n",
        "\n",
        "Now we use Elasticsearch's native `retrievers` parameter with `text_similarity_reranker`.\n",
        "\n",
        "This is the **recommended** way to do reranking in Elasticsearch:\n",
        "- Single request (no separate API call)\n",
        "- Native integration\n",
        "- Optimized for performance\n",
        "\n",
        "**How it works:**\n",
        "1. Inner retriever: `standard` query gets initial candidates\n",
        "2. Outer retriever: `text_similarity_reranker` re-scores using the reranker model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def search_with_reranker(es_client, index: str, query: str, \n",
        "                         reranker_id: str, initial_size: int = 50, \n",
        "                         final_size: int = 10) -> list:\n",
        "    \"\"\"\n",
        "    Perform semantic search with native reranking via retrievers API.\n",
        "    \n",
        "    Args:\n",
        "        es_client: Elasticsearch client\n",
        "        index: Index name\n",
        "        query: Search query\n",
        "        reranker_id: Inference endpoint ID for reranker\n",
        "        initial_size: Number of candidates for reranking\n",
        "        final_size: Final number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        List of hit dictionaries\n",
        "    \"\"\"\n",
        "    results = es_client.search(\n",
        "        index=index,\n",
        "        retriever={\n",
        "            \"text_similarity_reranker\": {\n",
        "                \"retriever\": {\n",
        "                    \"standard\": {\n",
        "                        \"query\": {\n",
        "                            \"semantic\": {\n",
        "                                \"field\": \"text\",\n",
        "                                \"query\": query\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                \"field\": \"text\",\n",
        "                \"inference_id\": reranker_id,\n",
        "                \"inference_text\": query,\n",
        "                \"rank_window_size\": initial_size,\n",
        "            }\n",
        "        },\n",
        "        size=final_size,\n",
        "        source=[\"article_number\", \"title\", \"text\"],\n",
        "    )\n",
        "    return results['hits']['hits']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get reranked results\n",
        "print(\"Running reranked search...\")\n",
        "print(f\"  Initial candidates: 50\")\n",
        "print(f\"  Reranker: {RERANKER_ID}\")\n",
        "\n",
        "_t0 = time.time()\n",
        "reranked_results = search_with_reranker(\n",
        "    es, INDEX_NAME, QUERY, \n",
        "    reranker_id=RERANKER_ID,\n",
        "    initial_size=50,\n",
        "    final_size=10\n",
        ")\n",
        "reranked_elapsed_ms = (time.time() - _t0) * 1000\n",
        "\n",
        "print(f\"\\n--- Reranked Results ({reranked_elapsed_ms:.0f} ms) ---\")\n",
        "print(f\"Query: \\\"{QUERY}\\\"\\n\")\n",
        "\n",
        "reranked_data = []\n",
        "for i, hit in enumerate(reranked_results, 1):\n",
        "    reranked_data.append({\n",
        "        \"Rank\": i,\n",
        "        \"Article\": hit['_source']['article_number'],\n",
        "        \"Title\": hit['_source']['title'][:50] + \"...\" if len(hit['_source']['title']) > 50 else hit['_source']['title'],\n",
        "        \"Score\": f\"{hit['_score']:.4f}\"\n",
        "    })\n",
        "\n",
        "df_reranked = pd.DataFrame(reranked_data)\n",
        "print(df_reranked.to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Side-by-Side Comparison\n",
        "\n",
        "Let's visualize how the rankings changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from utils.comparison import build_comparison"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display comparison\n",
        "print(\"=\" * 70)\n",
        "print(\"  RANKING COMPARISON: Naive vs Reranked\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Query: \\\"{QUERY}\\\"\")\n",
        "print()\n",
        "\n",
        "df_comparison = build_comparison(naive_results, reranked_results)\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "print()\n",
        "print(\"Legend: ↑ = moved up, ↓ = moved down, = = same, NEW = wasn't in top 10\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "assert len(df_comparison) == 10, f\"Expected 10 comparison rows, got {len(df_comparison)}\"\n",
        "assert \"Movement\" in df_comparison.columns\n",
        "print(\"✓ Comparison validation passed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Deep Dive: Why Reranking Matters\n",
        "\n",
        "Let's look at the actual text to understand why the reranker made different choices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show text snippets from top 3 reranked results\n",
        "print(\"=\" * 70)\n",
        "print(\"  TOP 3 RERANKED RESULTS - Why They Ranked Higher\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, hit in enumerate(reranked_results[:3], 1):\n",
        "    print(f\"\\n#{i}: Article {hit['_source']['article_number']} - {hit['_source']['title']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Get text content - handle semantic_text structure\n",
        "    text = hit['_source']['text']\n",
        "    if isinstance(text, dict):\n",
        "        text = text.get('text', str(text))\n",
        "    \n",
        "    # Show first 400 chars\n",
        "    preview = text[:400] if len(text) > 400 else text\n",
        "    print(preview + \"...\" if len(text) > 400 else preview)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quantitative Analysis\n",
        "\n",
        "Let's measure the impact of reranking on our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate reranking impact metrics\n",
        "naive_articles = [h['_source']['article_number'] for h in naive_results]\n",
        "reranked_articles = [h['_source']['article_number'] for h in reranked_results]\n",
        "\n",
        "# How many articles changed position?\n",
        "position_changes = sum(1 for i, art in enumerate(reranked_articles) \n",
        "                       if i < len(naive_articles) and naive_articles[i] != art)\n",
        "\n",
        "# How many new articles appeared in top 10?\n",
        "new_in_top10 = len(set(reranked_articles) - set(naive_articles))\n",
        "\n",
        "# Overlap between naive and reranked top 5\n",
        "top5_overlap = len(set(naive_articles[:5]) & set(reranked_articles[:5]))\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"  RERANKING IMPACT METRICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Position changes in top 10:  {position_changes}/10\")\n",
        "print(f\"  New articles in top 10:      {new_in_top10}\")\n",
        "print(f\"  Top-5 overlap:               {top5_overlap}/5\")\n",
        "print(\"=\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: The Full Chain Advantage\n",
        "\n",
        "You've now seen the complete **\"Full Chain\"** search pipeline:\n",
        "\n",
        "| Step | Tool | What It Does |\n",
        "|------|------|-------------|\n",
        "| **1. Ingest** | Jina Reader (ReaderLM) | PDF → Clean Markdown |\n",
        "| **2. Embed** | Jina Embeddings v5 (EIS) | Text → Semantic Vectors |\n",
        "| **3. Rerank** | Jina Reranker v3 (EIS) | Precision via Listwise Attention |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "| Concept | What We Learned |\n",
        "|---------|----------------|\n",
        "| **Listwise Reranking** | Documents attend to each other, not just the query |\n",
        "| **retrievers API** | Native Elasticsearch integration for reranking |\n",
        "| **Precision vs Recall** | Vector search has recall, reranking adds precision |\n",
        "| **Legal Nuance** | Reranking surfaces exceptions and specifics |\n",
        "\n",
        "### The \"Innocenti\" Advantage\n",
        "\n",
        "For a legal compliance firm:\n",
        "- **Before:** \"Yes, facial recognition is banned\" (general answer)\n",
        "- **After:** \"Facial recognition is banned EXCEPT for law enforcement in specific circumstances\" (precise answer)\n",
        "\n",
        "That precision can be the difference between compliance and a violation.\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "**Layer B: The Demo UI**\n",
        "\n",
        "The Next.js application will:\n",
        "1. Connect to this same index\n",
        "2. Let users search the EU AI Act\n",
        "3. Have a \"Deep Analysis\" button that triggers reranking\n",
        "4. Visually show the ranking changes\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}