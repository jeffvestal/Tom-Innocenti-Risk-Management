{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 03: Rerank - \"The Precision Logic\"\n",
        "\n",
        "## Innocenti Risk Management Enablement Kit\n",
        "\n",
        "---\n",
        "\n",
        "### The Problem\n",
        "\n",
        "Vector search finds semantically similar documents, but it doesn't understand **nuance**:\n",
        "\n",
        "- \"Can police use facial recognition?\" matches both:\n",
        "  - Article 5 (General prohibition)\n",
        "  - Article 5.1.d (Specific exceptions for law enforcement)\n",
        "\n",
        "A lawyer needs the **specific exception**, not just \"similar\" documents.\n",
        "\n",
        "### The Solution: Listwise Reranking\n",
        "\n",
        "**Traditional Reranking (Pointwise):**\n",
        "- Scores each document independently against the query\n",
        "- Doesn't consider relationships between documents\n",
        "\n",
        "**Listwise Reranking (Jina Reranker v3):**\n",
        "- Documents \"attend\" to each other during scoring\n",
        "- Understands context: \"This document is more specific than that one\"\n",
        "- The \"Lawyer's Brain\" effect\n",
        "\n",
        "**What we'll do:**\n",
        "1. Retrieve top 50 results (\"naive\" retrieval)\n",
        "2. Apply Jina Reranker v3 via Elasticsearch's `retrievers` API\n",
        "3. Compare before/after rankings\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q elasticsearch python-dotenv pandas\n",
        "\n",
        "# Check if running in Google Colab\n",
        "import os\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_RELEASE_TAG' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"ðŸ“ Running in Google Colab\")\n",
        "    # Clone the repo if not already present\n",
        "    if not os.path.exists('/content/Tom-Innocenti-Risk-Management'):\n",
        "        !git clone https://github.com/jeffvestal/Tom-Innocenti-Risk-Management.git\n",
        "    # Change to notebooks directory\n",
        "    os.chdir('/content/Tom-Innocenti-Risk-Management/notebooks')\n",
        "    print(f\"   Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"ðŸ“ Running locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from elasticsearch import Elasticsearch, BadRequestError\n",
        "import pandas as pd\n",
        "\n",
        "# Import our credential helper\n",
        "# (Path is already set correctly in previous cell for both Colab and local)\n",
        "from utils.credentials import setup_notebook, get_index_name, get_inference_id\n",
        "\n",
        "# Better pandas display\n",
        "pd.set_option('display.max_colwidth', 60)\n",
        "pd.set_option('display.width', 200)\n",
        "\n",
        "print(\"âœ“ Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup credentials and display configuration\n",
        "creds = setup_notebook(require_elastic=True, require_jina=False)\n",
        "\n",
        "# Get unique names for this user\n",
        "INDEX_NAME = get_index_name(\"search-eu-ai-act\")\n",
        "EMBEDDING_ID = get_inference_id(\"embeddings\")\n",
        "RERANKER_ID = get_inference_id(\"reranker\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to Elasticsearch\n",
        "es = Elasticsearch(\n",
        "    cloud_id=creds[\"ELASTIC_CLOUD_ID\"],\n",
        "    api_key=creds[\"ELASTIC_API_KEY\"]\n",
        ")\n",
        "\n",
        "# Verify connection and index\n",
        "info = es.info()\n",
        "print(f\"âœ“ Connected to Elasticsearch {info['version']['number']}\")\n",
        "\n",
        "if es.indices.exists(index=INDEX_NAME):\n",
        "    count = es.count(index=INDEX_NAME)[\"count\"]\n",
        "    print(f\"âœ“ Index {INDEX_NAME} has {count} documents\")\n",
        "else:\n",
        "    raise ValueError(f\"Index {INDEX_NAME} not found. Run Notebook 02 first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Reranker Inference Endpoint\n",
        "\n",
        "We need to set up the Jina Reranker v3 model as an inference endpoint.\n",
        "\n",
        "**Jina Reranker v3 highlights:**\n",
        "- **Listwise** attention mechanism\n",
        "- Documents attend to each other, not just the query\n",
        "- Better at distinguishing \"general\" vs \"specific\" matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_reranker_inference(es_client, inference_id: str) -> bool:\n",
        "    \"\"\"\n",
        "    Create a Jina Reranker v3 inference endpoint.\n",
        "    \n",
        "    Handles ResourceAlreadyExists gracefully for idempotency.\n",
        "    \n",
        "    Args:\n",
        "        es_client: Elasticsearch client\n",
        "        inference_id: Unique ID for this inference endpoint\n",
        "    \n",
        "    Returns:\n",
        "        True if created, False if already existed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        es_client.inference.put(\n",
        "            inference_id=inference_id,\n",
        "            task_type=\"rerank\",\n",
        "            body={\n",
        "                \"service\": \"jinaai\",\n",
        "                \"service_settings\": {\n",
        "                    \"model_id\": \"jina-reranker-v2-base-multilingual\"\n",
        "                    # Note: API key can be set here or in Kibana\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        print(f\"âœ“ Created reranker endpoint: {inference_id}\")\n",
        "        return True\n",
        "        \n",
        "    except BadRequestError as e:\n",
        "        if \"resource_already_exists_exception\" in str(e).lower() or \"already exists\" in str(e).lower():\n",
        "            print(f\"âœ“ Reranker endpoint already exists: {inference_id}\")\n",
        "            return False\n",
        "        else:\n",
        "            raise e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the reranker inference endpoint\n",
        "create_reranker_inference(es, RERANKER_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Define Our Test Query\n",
        "\n",
        "We'll use a nuanced legal query that has both general and specific answers in the EU AI Act."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A nuanced query where context matters\n",
        "QUERY = \"Can law enforcement use facial recognition in public spaces?\"\n",
        "\n",
        "# What we expect:\n",
        "# - Article 5: General prohibition on biometric systems\n",
        "# - Article 5.1.d: EXCEPTIONS for law enforcement (the nuanced answer)\n",
        "\n",
        "print(f\"Test Query: \\\"{QUERY}\\\"\")\n",
        "print(\"\\nExpected behavior:\")\n",
        "print(\"  - Vector search: Returns general prohibition articles\")\n",
        "print(\"  - Reranked: Should surface the law enforcement EXCEPTIONS\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Naive Retrieval (Vector Search Only)\n",
        "\n",
        "First, let's see what pure semantic search returns - no reranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_naive(es_client, index: str, query: str, size: int = 10) -> list:\n",
        "    \"\"\"\n",
        "    Perform naive semantic search without reranking.\n",
        "    \n",
        "    Args:\n",
        "        es_client: Elasticsearch client\n",
        "        index: Index name\n",
        "        query: Search query\n",
        "        size: Number of results\n",
        "    \n",
        "    Returns:\n",
        "        List of hit dictionaries\n",
        "    \"\"\"\n",
        "    results = es_client.search(\n",
        "        index=index,\n",
        "        body={\n",
        "            \"query\": {\n",
        "                \"semantic\": {\n",
        "                    \"field\": \"text\",\n",
        "                    \"query\": query\n",
        "                }\n",
        "            },\n",
        "            \"size\": size,\n",
        "            \"_source\": [\"article_number\", \"title\", \"text\"]\n",
        "        }\n",
        "    )\n",
        "    return results['hits']['hits']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get naive results\n",
        "naive_results = search_naive(es, INDEX_NAME, QUERY, size=10)\n",
        "\n",
        "print(f\"--- Naive Semantic Search Results ---\")\n",
        "print(f\"Query: \\\"{QUERY}\\\"\\n\")\n",
        "\n",
        "naive_data = []\n",
        "for i, hit in enumerate(naive_results, 1):\n",
        "    naive_data.append({\n",
        "        \"Rank\": i,\n",
        "        \"Article\": hit['_source']['article_number'],\n",
        "        \"Title\": hit['_source']['title'][:50] + \"...\" if len(hit['_source']['title']) > 50 else hit['_source']['title'],\n",
        "        \"Score\": f\"{hit['_score']:.4f}\"\n",
        "    })\n",
        "\n",
        "df_naive = pd.DataFrame(naive_data)\n",
        "print(df_naive.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranked Retrieval (Using `retrievers` API)\n",
        "\n",
        "Now we use Elasticsearch's native `retrievers` parameter with `text_similarity_reranker`.\n",
        "\n",
        "This is the **recommended** way to do reranking in Elasticsearch:\n",
        "- Single request (no separate API call)\n",
        "- Native integration\n",
        "- Optimized for performance\n",
        "\n",
        "**How it works:**\n",
        "1. Inner retriever: `standard` query gets initial candidates\n",
        "2. Outer retriever: `text_similarity_reranker` re-scores using the reranker model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_with_reranker(es_client, index: str, query: str, \n",
        "                         reranker_id: str, initial_size: int = 50, \n",
        "                         final_size: int = 10) -> list:\n",
        "    \"\"\"\n",
        "    Perform semantic search with native reranking via retrievers API.\n",
        "    \n",
        "    Args:\n",
        "        es_client: Elasticsearch client\n",
        "        index: Index name\n",
        "        query: Search query\n",
        "        reranker_id: Inference endpoint ID for reranker\n",
        "        initial_size: Number of candidates for reranking\n",
        "        final_size: Final number of results to return\n",
        "    \n",
        "    Returns:\n",
        "        List of hit dictionaries\n",
        "    \"\"\"\n",
        "    results = es_client.search(\n",
        "        index=index,\n",
        "        body={\n",
        "            \"retriever\": {\n",
        "                \"text_similarity_reranker\": {\n",
        "                    \"retriever\": {\n",
        "                        # Inner retriever: semantic search\n",
        "                        \"standard\": {\n",
        "                            \"query\": {\n",
        "                                \"semantic\": {\n",
        "                                    \"field\": \"text\",\n",
        "                                    \"query\": query\n",
        "                                }\n",
        "                            }\n",
        "                        }\n",
        "                    },\n",
        "                    \"field\": \"text\",  # Field to rerank on\n",
        "                    \"inference_id\": reranker_id,\n",
        "                    \"inference_text\": query,\n",
        "                    \"rank_window_size\": initial_size  # How many to consider for reranking\n",
        "                }\n",
        "            },\n",
        "            \"size\": final_size,\n",
        "            \"_source\": [\"article_number\", \"title\", \"text\"]\n",
        "        }\n",
        "    )\n",
        "    return results['hits']['hits']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get reranked results\n",
        "print(\"Running reranked search...\")\n",
        "print(f\"  Initial candidates: 50\")\n",
        "print(f\"  Reranker: {RERANKER_ID}\")\n",
        "\n",
        "reranked_results = search_with_reranker(\n",
        "    es, INDEX_NAME, QUERY, \n",
        "    reranker_id=RERANKER_ID,\n",
        "    initial_size=50,\n",
        "    final_size=10\n",
        ")\n",
        "\n",
        "print(f\"\\n--- Reranked Results ---\")\n",
        "print(f\"Query: \\\"{QUERY}\\\"\\n\")\n",
        "\n",
        "reranked_data = []\n",
        "for i, hit in enumerate(reranked_results, 1):\n",
        "    reranked_data.append({\n",
        "        \"Rank\": i,\n",
        "        \"Article\": hit['_source']['article_number'],\n",
        "        \"Title\": hit['_source']['title'][:50] + \"...\" if len(hit['_source']['title']) > 50 else hit['_source']['title'],\n",
        "        \"Score\": f\"{hit['_score']:.4f}\"\n",
        "    })\n",
        "\n",
        "df_reranked = pd.DataFrame(reranked_data)\n",
        "print(df_reranked.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Side-by-Side Comparison\n",
        "\n",
        "Let's visualize how the rankings changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build comparison dataframe\n",
        "def build_comparison(naive_hits: list, reranked_hits: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build a side-by-side comparison of naive vs reranked results.\n",
        "    \"\"\"\n",
        "    # Create lookup for naive rankings\n",
        "    naive_ranks = {hit['_source']['article_number']: i+1 \n",
        "                   for i, hit in enumerate(naive_hits)}\n",
        "    \n",
        "    comparison = []\n",
        "    for i, hit in enumerate(reranked_hits, 1):\n",
        "        article = hit['_source']['article_number']\n",
        "        naive_rank = naive_ranks.get(article, \">10\")\n",
        "        \n",
        "        # Calculate movement\n",
        "        if isinstance(naive_rank, int):\n",
        "            movement = naive_rank - i\n",
        "            if movement > 0:\n",
        "                movement_str = f\"â†‘{movement}\"\n",
        "            elif movement < 0:\n",
        "                movement_str = f\"â†“{abs(movement)}\"\n",
        "            else:\n",
        "                movement_str = \"=\"\n",
        "        else:\n",
        "            movement_str = \"NEW\"\n",
        "        \n",
        "        comparison.append({\n",
        "            \"Reranked\": i,\n",
        "            \"Article\": article,\n",
        "            \"Title\": hit['_source']['title'][:40] + \"...\",\n",
        "            \"Was Rank\": naive_rank,\n",
        "            \"Movement\": movement_str\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(comparison)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display comparison\n",
        "print(\"=\" * 70)\n",
        "print(\"  RANKING COMPARISON: Naive vs Reranked\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Query: \\\"{QUERY}\\\"\")\n",
        "print()\n",
        "\n",
        "df_comparison = build_comparison(naive_results, reranked_results)\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "print()\n",
        "print(\"Legend: â†‘ = moved up, â†“ = moved down, = = same, NEW = wasn't in top 10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Deep Dive: Why Reranking Matters\n",
        "\n",
        "Let's look at the actual text to understand why the reranker made different choices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show text snippets from top 3 reranked results\n",
        "print(\"=\" * 70)\n",
        "print(\"  TOP 3 RERANKED RESULTS - Why They Ranked Higher\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, hit in enumerate(reranked_results[:3], 1):\n",
        "    print(f\"\\n#{i}: Article {hit['_source']['article_number']} - {hit['_source']['title']}\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    # Get text content - handle semantic_text structure\n",
        "    text = hit['_source']['text']\n",
        "    if isinstance(text, dict):\n",
        "        text = text.get('text', str(text))\n",
        "    \n",
        "    # Show first 400 chars\n",
        "    preview = text[:400] if len(text) > 400 else text\n",
        "    print(preview + \"...\" if len(text) > 400 else preview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Quantitative Analysis\n",
        "\n",
        "Let's measure the impact of reranking on our results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate reranking impact metrics\n",
        "naive_articles = [h['_source']['article_number'] for h in naive_results]\n",
        "reranked_articles = [h['_source']['article_number'] for h in reranked_results]\n",
        "\n",
        "# How many articles changed position?\n",
        "position_changes = sum(1 for i, art in enumerate(reranked_articles) \n",
        "                       if i < len(naive_articles) and naive_articles[i] != art)\n",
        "\n",
        "# How many new articles appeared in top 10?\n",
        "new_in_top10 = len(set(reranked_articles) - set(naive_articles))\n",
        "\n",
        "# Overlap between naive and reranked top 5\n",
        "top5_overlap = len(set(naive_articles[:5]) & set(reranked_articles[:5]))\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"  RERANKING IMPACT METRICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Position changes in top 10:  {position_changes}/10\")\n",
        "print(f\"  New articles in top 10:      {new_in_top10}\")\n",
        "print(f\"  Top-5 overlap:               {top5_overlap}/5\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary: The Full Chain Advantage\n",
        "\n",
        "You've now seen the complete **\"Full Chain\"** search pipeline:\n",
        "\n",
        "| Step | Tool | What It Does |\n",
        "|------|------|-------------|\n",
        "| **1. Ingest** | Jina Reader (ReaderLM) | PDF â†’ Clean Markdown |\n",
        "| **2. Embed** | Jina Embeddings v3 (EIS) | Text â†’ Semantic Vectors |\n",
        "| **3. Rerank** | Jina Reranker v3 (EIS) | Precision via Listwise Attention |\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "| Concept | What We Learned |\n",
        "|---------|----------------|\n",
        "| **Listwise Reranking** | Documents attend to each other, not just the query |\n",
        "| **retrievers API** | Native Elasticsearch integration for reranking |\n",
        "| **Precision vs Recall** | Vector search has recall, reranking adds precision |\n",
        "| **Legal Nuance** | Reranking surfaces exceptions and specifics |\n",
        "\n",
        "### The \"Innocenti\" Advantage\n",
        "\n",
        "For a legal compliance firm:\n",
        "- **Before:** \"Yes, facial recognition is banned\" (general answer)\n",
        "- **After:** \"Facial recognition is banned EXCEPT for law enforcement in specific circumstances\" (precise answer)\n",
        "\n",
        "That precision can be the difference between compliance and a violation.\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "**Layer B: The Demo UI**\n",
        "\n",
        "The Next.js application will:\n",
        "1. Connect to this same index\n",
        "2. Let users search the EU AI Act\n",
        "3. Have a \"Deep Analysis\" button that triggers reranking\n",
        "4. Visually show the ranking changes\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
