{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 01: Ingest & Clean - \"The Garbage-In Fix\"\n",
        "\n",
        "## Innocenti Risk Management Enablement Kit\n",
        "\n",
        "---\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before running this notebook, you'll need:\n",
        "\n",
        "1. **Jina API Key** (free tier available)\n",
        "   - Sign up at [jina.ai](https://jina.ai/api-dashboard/)\n",
        "   - Create an API key from the dashboard\n",
        "   - You'll be prompted to enter it when running the notebook\n",
        "\n",
        "---\n",
        "\n",
        "### About Jina ReaderLM\n",
        "\n",
        "[ReaderLM](https://jina.ai/reader/) is a vision-language model purpose-built for document reading:\n",
        "- **Visual understanding** - Processes documents as images, not raw text extraction\n",
        "- **Layout-aware** - Handles tables, columns, headers, footers intelligently\n",
        "- **Clean output** - Returns structured markdown, not messy OCR text\n",
        "- **No setup** - Simple API call, no model hosting required\n",
        "\n",
        "---\n",
        "\n",
        "### The Problem\n",
        "\n",
        "Legal documents like the **EU AI Act** are notoriously hard to search:\n",
        "\n",
        "1. **PDFs are messy** - Headers, footers, page numbers, and weird formatting\n",
        "2. **OCR is expensive** - Traditional extraction requires heavy compute\n",
        "3. **Context gets lost** - Naive chunking breaks legal clauses mid-sentence\n",
        "\n",
        "### The Solution: Jina Reader (ReaderLM)\n",
        "\n",
        "Jina Reader is a specialized model that \"sees\" document layout and extracts clean, structured text without traditional OCR.\n",
        "\n",
        "**What we'll do:**\n",
        "1. Fetch the EU AI Act PDF via Jina Reader API\n",
        "2. Parse the markdown output\n",
        "3. Intelligently chunk by **Article** (preserving legal context)\n",
        "4. Save structured JSON for indexing\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q requests python-dotenv\n",
        "\n",
        "# Check if running in Google Colab\n",
        "import os\n",
        "IN_COLAB = 'COLAB_GPU' in os.environ or 'COLAB_RELEASE_TAG' in os.environ\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"üìç Running in Google Colab\")\n",
        "    # Always start from /content to avoid \"directory not found\" issues\n",
        "    os.chdir('/content')\n",
        "    # Clone or update the repo\n",
        "    if not os.path.exists('/content/Tom-Innocenti-Risk-Management'):\n",
        "        !git clone https://github.com/jeffvestal/Tom-Innocenti-Risk-Management.git\n",
        "    else:\n",
        "        # Pull latest changes\n",
        "        !cd /content/Tom-Innocenti-Risk-Management && git pull\n",
        "    # Change to notebooks directory\n",
        "    os.chdir('/content/Tom-Innocenti-Risk-Management/notebooks')\n",
        "    print(f\"   Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"üìç Running locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our credential helper\n",
        "# (Path is already set correctly in previous cell for both Colab and local)\n",
        "from utils.credentials import setup_notebook, get_credentials\n",
        "\n",
        "print(\"‚úì Libraries loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup credentials (will prompt on first run)\n",
        "# For this notebook, we only need the Jina API key\n",
        "creds = get_credentials(require_elastic=False, require_jina=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fetch PDF via Jina Reader\n",
        "\n",
        "The Jina Reader API converts any URL to clean markdown. For PDFs, it uses ReaderLM to \"see\" the layout.\n",
        "\n",
        "**Key headers:**\n",
        "- `x-respond-with: markdown` - Get markdown output (vs. plain text)\n",
        "- `Authorization: Bearer <key>` - Your Jina API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# EU AI Act PDF URL (official EUR-Lex source)\n",
        "PDF_URL = \"https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32024R1689\"\n",
        "\n",
        "# Jina Reader endpoint\n",
        "READER_URL = f\"https://r.jina.ai/{PDF_URL}\"\n",
        "\n",
        "print(f\"Source PDF: {PDF_URL}\")\n",
        "print(f\"Reader URL: {READER_URL[:60]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def fetch_with_jina_reader(url: str, api_key: str, max_retries: int = 3) -> str:\n",
        "    \"\"\"\n",
        "    Fetch a URL via Jina Reader and return clean markdown.\n",
        "    \n",
        "    Includes retry logic for transient failures (empty responses).\n",
        "    \n",
        "    Args:\n",
        "        url: The Jina Reader URL (https://r.jina.ai/<target_url>)\n",
        "        api_key: Your Jina API key\n",
        "        max_retries: Number of retry attempts if response is empty\n",
        "    \n",
        "    Returns:\n",
        "        Clean markdown text\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"x-respond-with\": \"markdown\",\n",
        "        \"Accept\": \"text/plain\"\n",
        "    }\n",
        "    \n",
        "    print(\"Fetching PDF via Jina Reader...\")\n",
        "    print(\"(This may take 30-60 seconds for a large document)\")\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        response = requests.get(url, headers=headers, timeout=120)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        content = response.text.strip()\n",
        "        \n",
        "        # Check if we got actual content (not empty or just whitespace)\n",
        "        if len(content) > 100:\n",
        "            print(f\"‚úì Received {len(response.text):,} characters\")\n",
        "            return response.text\n",
        "        \n",
        "        # Empty response - retry\n",
        "        if attempt < max_retries - 1:\n",
        "            wait_time = (attempt + 1) * 5  # 5s, 10s, 15s\n",
        "            print(f\"‚ö† Empty response (attempt {attempt + 1}/{max_retries}). Retrying in {wait_time}s...\")\n",
        "            time.sleep(wait_time)\n",
        "        else:\n",
        "            print(f\"‚úó Failed after {max_retries} attempts - received empty content\")\n",
        "            raise ValueError(\n",
        "                \"Jina Reader returned empty content after multiple retries. \"\n",
        "                \"This can happen due to rate limiting. Wait a minute and try again.\"\n",
        "            )\n",
        "    \n",
        "    return response.text  # Shouldn't reach here, but just in case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch the document\n",
        "raw_markdown = fetch_with_jina_reader(READER_URL, creds[\"JINA_API_KEY\"])\n",
        "\n",
        "# Preview the first 1000 characters\n",
        "print(\"\\n--- Preview (first 1000 chars) ---\")\n",
        "print(raw_markdown[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Parse & Chunk by Article\n",
        "\n",
        "Legal documents have structure. The EU AI Act is organized into **Articles**. \n",
        "\n",
        "**Chunking Strategy:**\n",
        "- Split on `Article \\d+` pattern\n",
        "- Capture the article number and title\n",
        "- Keep entire article text together (no mid-sentence breaks)\n",
        "\n",
        "This preserves legal context that would be lost with naive character-based chunking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_articles(markdown_text: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Parse EU AI Act markdown into structured article chunks.\n",
        "    \n",
        "    Args:\n",
        "        markdown_text: Raw markdown from Jina Reader\n",
        "    \n",
        "    Returns:\n",
        "        List of article dictionaries with id, article_number, title, text, url\n",
        "    \"\"\"\n",
        "    articles = []\n",
        "    \n",
        "    # Pattern to match article headers\n",
        "    # Matches: \"Article 1\", \"Article 5\", \"## Article 10\", etc.\n",
        "    article_pattern = r'^(?:#+ )?Article\\s+(\\d+)\\s*\\n+([^\\n]+)?'\n",
        "    \n",
        "    # Split the document by article boundaries\n",
        "    splits = re.split(r'(?=^(?:#+ )?Article\\s+\\d+)', markdown_text, flags=re.MULTILINE)\n",
        "    \n",
        "    for chunk in splits:\n",
        "        if not chunk.strip():\n",
        "            continue\n",
        "            \n",
        "        # Try to extract article number and title\n",
        "        match = re.match(article_pattern, chunk, re.MULTILINE)\n",
        "        if match:\n",
        "            article_num = match.group(1)\n",
        "            # Title is the line after \"Article X\" (if present)\n",
        "            title_candidate = match.group(2) if match.group(2) else \"\"\n",
        "            title = title_candidate.strip() if title_candidate else f\"Article {article_num}\"\n",
        "            \n",
        "            # Get the body text (everything after the header)\n",
        "            body_start = match.end()\n",
        "            body = chunk[body_start:].strip()\n",
        "            \n",
        "            # Clean up the body text\n",
        "            body = re.sub(r'\\n{3,}', '\\n\\n', body)  # Collapse multiple newlines\n",
        "            body = body.strip()\n",
        "            \n",
        "            if body:  # Only add if there's actual content\n",
        "                articles.append({\n",
        "                    \"id\": f\"en_art_{article_num}\",\n",
        "                    \"article_number\": article_num,\n",
        "                    \"title\": title,\n",
        "                    \"text\": body,\n",
        "                    \"language\": \"en\",\n",
        "                    \"url\": f\"https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32024R1689#Art{article_num}\"\n",
        "                })\n",
        "    \n",
        "    return articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse the document into articles\n",
        "articles = parse_articles(raw_markdown)\n",
        "\n",
        "print(f\"‚úì Extracted {len(articles)} articles\")\n",
        "print(\"\\n--- Article Numbers Found ---\")\n",
        "print([a['article_number'] for a in articles[:20]], \"...\" if len(articles) > 20 else \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview a sample article (Article 5 - Prohibited Practices is a key one)\n",
        "sample_article = next((a for a in articles if a['article_number'] == '5'), articles[0])\n",
        "\n",
        "print(f\"--- Sample: Article {sample_article['article_number']} ---\")\n",
        "print(f\"Title: {sample_article['title']}\")\n",
        "print(f\"ID: {sample_article['id']}\")\n",
        "print(f\"\\nText (first 500 chars):\")\n",
        "print(sample_article['text'][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Structured JSON\n",
        "\n",
        "We'll save the parsed articles as JSON for use in Notebook 02 (Indexing).\n",
        "\n",
        "**Output Schema:**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"en_art_5\",\n",
        "  \"article_number\": \"5\",\n",
        "  \"title\": \"Prohibited artificial intelligence practices\",\n",
        "  \"text\": \"The following AI practices shall be prohibited...\",\n",
        "  \"language\": \"en\",\n",
        "  \"url\": \"https://eur-lex.europa.eu/...\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory if it doesn't exist\n",
        "# Works for both local (notebooks/../data) and Colab (/content/.../data)\n",
        "output_dir = Path.cwd().parent / \"data\"\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "output_file = output_dir / \"eu_ai_act_clean.json\"\n",
        "\n",
        "# Save to JSON\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(articles, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"‚úì Saved {len(articles)} articles to {output_file}\")\n",
        "print(f\"  File size: {output_file.stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "# In Colab, also save to /content for easy access\n",
        "if 'IN_COLAB' in dir() and IN_COLAB:\n",
        "    colab_output = Path('/content/eu_ai_act_clean.json')\n",
        "    with open(colab_output, 'w', encoding='utf-8') as f:\n",
        "        json.dump(articles, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"‚úì Also saved to {colab_output} (for easy Colab access)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Verification & Stats\n",
        "\n",
        "Let's verify the output and gather some statistics about our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate statistics\n",
        "total_chars = sum(len(a['text']) for a in articles)\n",
        "avg_chars = total_chars / len(articles) if articles else 0\n",
        "min_chars = min(len(a['text']) for a in articles) if articles else 0\n",
        "max_chars = max(len(a['text']) for a in articles) if articles else 0\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"  EU AI Act Dataset Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Total Articles:     {len(articles)}\")\n",
        "print(f\"  Total Characters:   {total_chars:,}\")\n",
        "print(f\"  Avg per Article:    {avg_chars:,.0f} chars\")\n",
        "print(f\"  Smallest Article:   {min_chars:,} chars\")\n",
        "print(f\"  Largest Article:    {max_chars:,} chars\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show the top 5 longest articles (usually the most important)\n",
        "sorted_by_length = sorted(articles, key=lambda x: len(x['text']), reverse=True)\n",
        "\n",
        "print(\"\\n--- Top 5 Longest Articles ---\")\n",
        "for i, article in enumerate(sorted_by_length[:5], 1):\n",
        "    print(f\"{i}. Article {article['article_number']}: {article['title'][:50]}... ({len(article['text']):,} chars)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "You've successfully:\n",
        "1. ‚úÖ Fetched the EU AI Act PDF via Jina Reader\n",
        "2. ‚úÖ Parsed it into structured article chunks\n",
        "3. ‚úÖ Saved clean JSON for indexing\n",
        "\n",
        "**Continue to Notebook 02** to index this data in Elasticsearch with `semantic_text` and Jina Embeddings v3.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "| Concept | What We Learned |\n",
        "|---------|----------------|\n",
        "| **ReaderLM** | Jina Reader \"sees\" PDF layout without OCR |\n",
        "| **Smart Chunking** | Split by semantic boundaries (Articles), not character count |\n",
        "| **Metadata Preservation** | Keep article numbers, titles, URLs for filtering & display |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
