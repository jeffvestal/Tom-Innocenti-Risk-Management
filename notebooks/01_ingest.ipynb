{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 01: Ingest & Clean - \"The Garbage-In Fix\"\n",
        "\n",
        "## Innocenti Risk Management Enablement Kit\n",
        "\n",
        "---\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before running this notebook, you'll need:\n",
        "\n",
        "1. **Jina API Key** (free tier available)\n",
        "   - Sign up at [jina.ai](https://jina.ai/api-dashboard/)\n",
        "   - Create an API key from the dashboard\n",
        "   - You'll be prompted to enter it when running the notebook\n",
        "\n",
        "---\n",
        "\n",
        "### About Jina ReaderLM\n",
        "\n",
        "[ReaderLM](https://jina.ai/reader/) is a vision-language model purpose-built for document reading:\n",
        "- **Visual understanding** - Processes documents as images, not raw text extraction\n",
        "- **Layout-aware** - Handles tables, columns, headers, footers intelligently\n",
        "- **Clean output** - Returns structured markdown, not messy OCR text\n",
        "- **No setup** - Simple API call, no model hosting required\n",
        "\n",
        "---\n",
        "\n",
        "### The Problem\n",
        "\n",
        "Legal documents like the **EU AI Act** are notoriously hard to search:\n",
        "\n",
        "1. **PDFs are messy** - Headers, footers, page numbers, and weird formatting\n",
        "2. **OCR is expensive** - Traditional extraction requires heavy compute\n",
        "3. **Context gets lost** - Naive chunking breaks legal clauses mid-sentence\n",
        "\n",
        "### The Solution: Jina Reader (ReaderLM)\n",
        "\n",
        "Jina Reader is a specialized model that \"sees\" document layout and extracts clean, structured text without traditional OCR.\n",
        "\n",
        "**What we'll do:**\n",
        "1. Fetch the EU AI Act PDF via Jina Reader API\n",
        "2. Parse the markdown output\n",
        "3. Intelligently chunk by **Article** (preserving legal context)\n",
        "4. Save structured JSON for indexing\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q requests python-dotenv\n",
        "\n",
        "from utils.colab_setup import setup_environment\n",
        "IN_COLAB = setup_environment(packages=\"requests python-dotenv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "import re\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# Import our credential helper\n",
        "# (Path is already set correctly in previous cell for both Colab and local)\n",
        "from utils.credentials import setup_notebook, get_credentials\n",
        "\n",
        "print(\"✓ Libraries loaded successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setup credentials (will prompt on first run)\n",
        "# For this notebook, we only need the Jina API key\n",
        "creds = get_credentials(require_elastic=False, require_jina=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Fetch PDF via Jina Reader\n",
        "\n",
        "The Jina Reader API converts any URL to clean markdown. For PDFs, it uses ReaderLM to \"see\" the layout.\n",
        "\n",
        "**Key headers:**\n",
        "- `x-respond-with: markdown` - Get markdown output (vs. plain text)\n",
        "- `Authorization: Bearer <key>` - Your Jina API key"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# EU AI Act PDF URL (official EUR-Lex source)\n",
        "PDF_URL = \"https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:32024R1689\"\n",
        "\n",
        "# Jina Reader endpoint\n",
        "READER_URL = f\"https://r.jina.ai/{PDF_URL}\"\n",
        "\n",
        "print(f\"Source PDF: {PDF_URL}\")\n",
        "print(f\"Reader URL: {READER_URL[:60]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from utils.reader import fetch_with_jina_reader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fetch the document\n",
        "raw_markdown = fetch_with_jina_reader(READER_URL, creds[\"JINA_API_KEY\"])\n",
        "\n",
        "# Preview the first 1000 characters\n",
        "print(\"\\n--- Preview (first 1000 chars) ---\")\n",
        "print(raw_markdown[:1000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Parse & Chunk by Article\n",
        "\n",
        "Legal documents have structure. The EU AI Act is organized into **Articles**. \n",
        "\n",
        "**Chunking Strategy:**\n",
        "- Split on `Article \\d+` pattern\n",
        "- Capture the article number and title\n",
        "- Keep entire article text together (no mid-sentence breaks)\n",
        "\n",
        "This preserves legal context that would be lost with naive character-based chunking."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from utils.parsing import parse_articles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Parse the document into articles\n",
        "articles = parse_articles(raw_markdown)\n",
        "\n",
        "print(f\"✓ Extracted {len(articles)} articles\")\n",
        "print(\"\\n--- Article Numbers Found ---\")\n",
        "print([a['article_number'] for a in articles[:20]], \"...\" if len(articles) > 20 else \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Preview a sample article (Article 5 - Prohibited Practices is a key one)\n",
        "sample_article = next((a for a in articles if a['article_number'] == '5'), articles[0])\n",
        "\n",
        "print(f\"--- Sample: Article {sample_article['article_number']} ---\")\n",
        "print(f\"Title: {sample_article['title']}\")\n",
        "print(f\"ID: {sample_article['id']}\")\n",
        "print(f\"\\nText (first 500 chars):\")\n",
        "print(sample_article['text'][:500])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Structured JSON\n",
        "\n",
        "We'll save the parsed articles as JSON for use in Notebook 02 (Indexing).\n",
        "\n",
        "**Output Schema:**\n",
        "```json\n",
        "{\n",
        "  \"id\": \"en_art_5\",\n",
        "  \"article_number\": \"5\",\n",
        "  \"title\": \"Prohibited artificial intelligence practices\",\n",
        "  \"text\": \"The following AI practices shall be prohibited...\",\n",
        "  \"language\": \"en\",\n",
        "  \"url\": \"https://eur-lex.europa.eu/...\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create output directory if it doesn't exist\n",
        "# Works for both local (notebooks/../data) and Colab (/content/.../data)\n",
        "output_dir = Path.cwd().parent / \"data\"\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "output_file = output_dir / \"eu_ai_act_clean.json\"\n",
        "\n",
        "# Save to JSON\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(articles, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"✓ Saved {len(articles)} articles to {output_file}\")\n",
        "print(f\"  File size: {output_file.stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "# In Colab, also save to /content for easy access\n",
        "if 'IN_COLAB' in dir() and IN_COLAB:\n",
        "    colab_output = Path('/content/eu_ai_act_clean.json')\n",
        "    with open(colab_output, 'w', encoding='utf-8') as f:\n",
        "        json.dump(articles, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"✓ Also saved to {colab_output} (for easy Colab access)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Verification & Stats\n",
        "\n",
        "Let's verify the output and gather some statistics about our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate statistics\n",
        "total_chars = sum(len(a['text']) for a in articles)\n",
        "avg_chars = total_chars / len(articles) if articles else 0\n",
        "min_chars = min(len(a['text']) for a in articles) if articles else 0\n",
        "max_chars = max(len(a['text']) for a in articles) if articles else 0\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"  EU AI Act Dataset Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"  Total Articles:     {len(articles)}\")\n",
        "print(f\"  Total Characters:   {total_chars:,}\")\n",
        "print(f\"  Avg per Article:    {avg_chars:,.0f} chars\")\n",
        "print(f\"  Smallest Article:   {min_chars:,} chars\")\n",
        "print(f\"  Largest Article:    {max_chars:,} chars\")\n",
        "print(\"=\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show the top 5 longest articles (usually the most important)\n",
        "sorted_by_length = sorted(articles, key=lambda x: len(x['text']), reverse=True)\n",
        "\n",
        "print(\"\\n--- Top 5 Longest Articles ---\")\n",
        "for i, article in enumerate(sorted_by_length[:5], 1):\n",
        "    print(f\"{i}. Article {article['article_number']}: {article['title'][:50]}... ({len(article['text']):,} chars)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "You've successfully:\n",
        "1. ✅ Fetched the EU AI Act PDF via Jina Reader\n",
        "2. ✅ Parsed it into structured article chunks\n",
        "3. ✅ Saved clean JSON for indexing\n",
        "\n",
        "**Continue to Notebook 02** to index this data in Elasticsearch with `semantic_text` and Jina Embeddings v5.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "| Concept | What We Learned |\n",
        "|---------|----------------|\n",
        "| **ReaderLM** | Jina Reader \"sees\" PDF layout without OCR |\n",
        "| **Smart Chunking** | Split by semantic boundaries (Articles), not character count |\n",
        "| **Metadata Preservation** | Keep article numbers, titles, URLs for filtering & display |"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}